# -*- coding: utf-8 -*-
"""Create_blog_post_using_AI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tvyDP-_QxyA0lDX-UOxbNoED72_AlFG1

# ***Inputs***
"""

SPREADSHEET_ID = "1d25lrI_5E4JvEf0VJUrNBXACQQ0Pq9moxaOxVEx8EMM"
FOLDER_ID_TRN = "1odqR5m3iyIrqI43U4FZ6qSUUmuV-8W23"
FOLDER_ID_SUM_BLOG = "1q5ysVCgw4c6b7YHuItjBAaVWPjZ3Y_5u"

system_prompt_first_part = (
    "SYSTEM PROMPT: You are an expert blog post writer who specializes in creating high-impact, educational blog posts for students of the Short-Term Rental industry. You have the unique ability to analyze transcription content and generate comprehensive blog post that include meticulous details."
    "INSTRUCTIONS: Analyze the full content of this transcript. "
    "[1] Create 5 compelling summary titles."
    "[2] Summarize the transcript using complete sentences, proper punctuation, and grammar in 150 words or less."
    "[3] Generate a comprehensive and verbose summary with much more detail than the short summary generated from Step 2."
    "[4] Identify 5 key points."
    "[5] Identify 5 action items."
)

system_prompt_second_part = (
    "SYSTEM PROMPT: You are an expert blog post writer who specializes in creating high-impact, educational blog posts for students of the Short-Term Rental industry. You have the unique ability to analyze transcription content and generate comprehensive blog post that include meticulous details."
    "INSTRUCTIONS: Analyze the full content of this transcript. Generate blog post with proper formatting."
    "[1] Identify up to 5 topics that can be used for value-added blog posts."
    "[2] Write a brief outline for each of the blog post topics from Step 1 (do not skip any blog post topics.)"
    "[3] For each individual blog post topic identified in Step 1, please write a separate full 500-word blog post for each topics based on Step 2 outlines. Use proper blog post writing format, use list, quote etc if needed. (do not skip any blog post topics)"
)

import os
from getpass import getpass
os.environ['OPENAI_API_KEY'] = getpass("Enter your OpenAI API key: ")

"""# ***All functions***

### ***Overall setup***
"""

# For working with Google Drive, Sheets, and Docs
!pip install gspread pandas google-auth google-api-python-client google-auth-httplib2 google-auth-oauthlib python-dateutil requests pydub python-docx

# Data Manipulation Libraries
import pandas as pd
import csv
import io
import json

# Time and Date Libraries
from time import time, sleep
import time
from datetime import datetime, timedelta, timezone
from zoneinfo import ZoneInfo # Import ZoneInfo from zoneinfo module
from dateutil import parser
from dateutil.relativedelta import relativedelta
import math

# Networking, API Clients, and Requests
import requests
import base64

# Google and Cloud Authentication/Services
import google.auth
from google.colab import auth
from google.colab import drive
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload, MediaFileUpload, MediaIoBaseUpload
from google.auth.transport.requests import Request
from google.oauth2.service_account import Credentials
from oauth2client.client import GoogleCredentials
from google_auth_oauthlib.flow import InstalledAppFlow
from google.oauth2 import service_account
import gspread
from collections import Counter
from collections import defaultdict
from googleapiclient.errors import HttpError
from docx import Document

from openai import OpenAI
client = OpenAI()

"""### ***Google Drive & Sheets***"""

# Initialize Google API clients
def initialize_google_creds():
    try:
        # Authenticate the user and get credentials
        auth.authenticate_user()
        credentials, _ = google.auth.default()

        # Initialize API clients
        docs_service = build('docs', 'v1', credentials=credentials)
        drive_service = build('drive', 'v3', credentials=credentials)
        sheets_service = build('sheets', 'v4', credentials=credentials)

        # Open the Google Sheets and get data from the required worksheets
        try:
            # Access the spreadsheet
            spreadsheet = sheets_service.spreadsheets().get(spreadsheetId=SPREADSHEET_ID).execute()
            print(f"Spreadsheet opened successfully! Title: {spreadsheet['properties']['title']}\n")

            return docs_service, drive_service, sheets_service

        except gspread.exceptions.SpreadsheetNotFound:
            raise Exception(f"Error: Spreadsheet not found with ID: {SPREADSHEET_ID}\n")

    except Exception as e:
        raise Exception(f"Error initializing credentials or services: {e}\n")


################################################################################


# Function to find and display duplicate primary_keys with filtering
def find_duplicate_primary_keys(sheets_service, spreadsheet_id, range_name, primary_key_col_name, filter_col_name, filter_value):
    # Finds and prints all duplicate primary_keys where filter_col_name equals filter_value.
    try:
        # Define the range to fetch (from row 3 onwards)
        range_to_fetch = f"{range_name}!A3:ZZZ"

        # Get values from the worksheet starting from row 3
        worksheet = sheets_service.spreadsheets().values().get(
            spreadsheetId=spreadsheet_id,
            range=range_to_fetch
        ).execute()
        data = worksheet.get('values', [])
        if not data:
            print(f"The {range_name} worksheet is empty.")
            return

        # Extract the header row (from data[0], which corresponds to row 3)
        header = data[0]

        # Find the index of the primary_key column (case-insensitive)
        try:
            primary_key_col_index = next(
                i for i, col_name in enumerate(header)
                if col_name.strip().lower() == primary_key_col_name.strip().lower()
            )
        except StopIteration:
            raise Exception(f"Column '{primary_key_col_name}' not found in the header.")

        # Find the index of the filter_col_name (case-insensitive)
        try:
            filter_col_index = next(
                i for i, col_name in enumerate(header)
                if col_name.strip().lower() == filter_col_name.strip().lower()
            )
        except StopIteration:
            raise Exception(f"Column '{filter_col_name}' not found in the header.")

        # Ensure there are enough rows to start from row 5
        if len(data) < 3:
            print("Not enough data to process.")
            return

        # Extract primary_keys where filter_col_name equals filter_value (starting from data[2], which corresponds to row 5)
        primary_keys = [
            row[primary_key_col_index]
            for row in data[2:]  # data[2] corresponds to row 5
            if len(row) > max(primary_key_col_index, filter_col_index)
               and row[filter_col_index].strip() == filter_value
               and row[primary_key_col_index]
        ]

        # Count occurrences of each primary_key
        primary_key_counts = Counter(primary_keys)

        # Find duplicates (primary_keys with a count greater than 1)
        duplicates = {primary_key: count for primary_key, count in primary_key_counts.items() if count > 1}

        if duplicates:
            print("Duplicate primary_keys found:\n")
            for primary_key, count in duplicates.items():
                print(f"Primary key: '{primary_key}', Found: {count} times")
        else:
            print("No duplicate primary_keys found.")

        return data, header, primary_keys, duplicates

    except Exception as e:
        raise Exception(f"Error finding duplicate primary_keys: {e}")

# Retrieve All Files in the Specified Folder
def get_files_in_folder(drive_service, GDRIVE_FOLDER_ID):
    print(f"Retrieving files from https://drive.google.com/drive/u/0/folders/{GDRIVE_FOLDER_ID}/")
    files = []
    page_token = None

    try:
        while True:
            response = drive_service.files().list(
                q=f"'{GDRIVE_FOLDER_ID}' in parents and trashed = false",
                spaces='drive',
                fields='nextPageToken, files(id, name, mimeType, modifiedTime, size)',
                pageToken=page_token
            ).execute()

            files.extend(response.get('files', []))
            page_token = response.get('nextPageToken', None)

            if page_token is None:
                break

        if files:
            print(f"Successfully connected with Google Drive API. Total files retrieved: {len(files)}\n")

        return files

    except Exception as e:
        raise Exception(f"Error retrieving files from Google Drive: {e}\n")


################################################################################


# Identify Duplicate File Names
def find_duplicate_files(drive_service, GDRIVE_FOLDER_ID, files):
    name_to_files = defaultdict(list)
    for file in files:
        name_to_files[file['name']].append(file)

    duplicates = {name: file_list for name, file_list in name_to_files.items() if len(file_list) > 1}

    data = []

    for name, files in duplicates.items():
        for file in files:
            data.append({
                'File Name': name,
                'File ID': file['id'],
                'MIME Type': file['mimeType'],
                'Modified Time': file['modifiedTime'],
                'Size (bytes)': file.get('size', 'N/A')
            })

    df_duplicates = None  # Initialize to None

    if data:
        print(f"Duplicate file names found into this folder: https://drive.google.com/drive/u/0/folders/{GDRIVE_FOLDER_ID}/. Displaying duplicates in a table format: ")
        df_duplicates = pd.DataFrame(data)
    else:
        print(f"No duplicate file names found into this folder: https://drive.google.com/drive/u/0/folders/{GDRIVE_FOLDER_ID}/\n")

    return df_duplicates

"""### **Main function**"""

def check_transcript_file_names(primary_keys, gdrive_files):
    """Check for transcript file names in Google Drive."""

    existing_file_id = []

    # Create a mapping from lowercased trn file names to lists of files with that name
    name_to_files = defaultdict(list)
    for file in gdrive_files:
        name_to_files[file['name'].lower()].append(file)

    # Process each primary key
    for file_name in primary_keys:
        # Expected trn file name in lowercase
        transcript_file_name = file_name.lower() + " trn"
        files_with_transcript_name = name_to_files.get(transcript_file_name, [])
        count = len(files_with_transcript_name)

        if count == 0:
            print(f"File not found in GDrive: '{transcript_file_name}'\n")
        elif count > 1:
            print(f"Duplicate files found for: '{transcript_file_name}'\n")
        else:
            file_id = files_with_transcript_name[0]['id']
            existing_file_id.append(file_id)

    return existing_file_id

# Function to process files
def process_files_in_folder(docs_service, drive_service, folder_id, gdrive_files, file_ids):
    """Process files in a folder based on their IDs."""

    # Search for files in the specified folder
    files = gdrive_files

    # Filter files based on file_ids
    target_files = [file for file in files if file['id'] in file_ids]

    for file in target_files:
        file_id = file['id']
        file_name = file['name']
        mime_type = file['mimeType']

        print(f"Processing file: '{file_name}'    ({file_id})")

        # Check if the file is a Google Docs document
        if mime_type == 'application/vnd.google-apps.document':
            # Retrieve the document content using Docs API
            document = docs_service.documents().get(documentId=file_id).execute()
            content = read_paragraph_elements(document.get('body').get('content'))

            # Split content into words
            words = content.strip().split()
            if words and words[0] == 'WEBVTT':
                print(f"'WEBVTT' found as the first word in '{file_name}'. Processing...")
                # Process the content
                new_content = extract_paragraph_from_webvtt(content)

                # Update the Google Docs file with the new content
                update_google_doc(docs_service, file_id, new_content)
                print(f"Updated Google Docs file: {file_name}\n")
            else:
                print(f"Perfect. 'WEBVTT' not found as the first word in '{file_name}'. Skipping.\n")

        # Check if the file is a plain text file
        elif mime_type == 'text/plain':
            # Download the text file
            request = drive_service.files().get_media(fileId=file_id)
            fh = io.BytesIO()
            downloader = MediaIoBaseDownload(fh, request)
            done = False
            while not done:
                status, done = downloader.next_chunk()
            content = fh.getvalue().decode('utf-8')

            # Split content into words
            words = content.strip().split()
            if words and words[0] == 'WEBVTT':
                print(f"'WEBVTT' found as the first word in '{file_name}'. Processing...")
                # Process the content
                new_content = extract_paragraph_from_webvtt(content)

                # Upload the new content back to the text file
                fh = io.BytesIO(new_content.encode('utf-8'))
                media_body = MediaIoBaseUpload(fh, mimetype='text/plain', resumable=True)
                drive_service.files().update(fileId=file_id, media_body=media_body).execute()
                print(f"Updated text file: {file_name}\n")
            else:
                print(f"Perfect. 'WEBVTT' not found as the first word in '{file_name}'. Skipping.\n")
        elif mime_type == 'application/vnd.openxmlformats-officedocument.wordprocessingml.document':
            # Download the DOCX file
            request = drive_service.files().get_media(fileId=file_id)
            fh = io.BytesIO()
            downloader = MediaIoBaseDownload(fh, request)
            done = False
            while not done:
                status, done = downloader.next_chunk()
            fh.seek(0)

            # Read the DOCX file using python-docx
            document = Document(fh)
            full_text = []
            for para in document.paragraphs:
                full_text.append(para.text)
            content = '\n'.join(full_text)

            # Split content into words
            words = content.strip().split()
            if words and words[0] == 'WEBVTT':
                print(f"'WEBVTT' found as the first word in '{file_name}'. Processing...")
                # Process the content
                new_content = extract_paragraph_from_webvtt(content)

                # Remove all existing paragraphs
                for para in document.paragraphs:
                    p = para._element
                    p.getparent().remove(p)
                    para._p = para._element = None

                # Add the new content
                document.add_paragraph(new_content)

                # Save the modified DOCX to a BytesIO object
                output = io.BytesIO()
                document.save(output)
                output.seek(0)

                # Upload the modified DOCX file back to Google Drive
                media_body = MediaIoBaseUpload(
                    output,
                    mimetype='application/vnd.openxmlformats-officedocument.wordprocessingml.document',
                    resumable=True
                )
                drive_service.files().update(fileId=file_id, media_body=media_body).execute()
                print(f"Updated DOCX file: {file_name}\n")
            else:
                print(f"Perfect. 'WEBVTT' not found as the first word in '{file_name}'. Skipping.\n")
        else:
            print(f"Unsupported file type for '{file_name}'. Skipping.\n")


################################################################################


# Function to read the content of a Google Docs document
def read_paragraph_elements(elements):
    text = ''
    for value in elements:
        if 'paragraph' in value:
            paragraph_elements = value.get('paragraph').get('elements')
            for elem in paragraph_elements:
                if 'textRun' in elem:
                    text += elem.get('textRun').get('content')
    return text


################################################################################


# Function to extract paragraph from WEBVTT content
def extract_paragraph_from_webvtt(content):
    lines = content.splitlines()
    text_lines = []
    for line in lines:
        # Skip timecodes, numbering, and 'WEBVTT'
        if '-->' in line or line.strip().isdigit() or not line.strip() or line.strip() == 'WEBVTT':
            continue
        text_lines.append(line.strip())
    # Combine the text lines into a single paragraph
    paragraph = ' '.join(text_lines)
    return paragraph


################################################################################


# Function to update a Google Docs document
def update_google_doc(docs_service, document_id, new_content):
    # Get the document to find its end index
    doc = docs_service.documents().get(documentId=document_id).execute()
    body_content = doc.get('body').get('content')

    # Find the end index of the document
    if body_content:
        last_element = body_content[-1]
        if 'endIndex' in last_element:
            doc_end_index = last_element['endIndex']
        else:
            # If 'endIndex' is not in the last element, search backwards
            for element in reversed(body_content):
                if 'endIndex' in element:
                    doc_end_index = element['endIndex']
                    break
            else:
                # If no 'endIndex' is found, set it to 1
                doc_end_index = 1
    else:
        # Empty document
        doc_end_index = 1

    # Clear the document content
    requests = [
        {
            'deleteContentRange': {
                'range': {
                    'startIndex': 1,
                    'endIndex': doc_end_index - 1  # Subtract 1 to stay within the document bounds
                }
            }
        },
        {
            'insertText': {
                'location': {
                    'index': 1
                },
                'text': new_content
            }
        }
    ]

    # Execute the batchUpdate request
    docs_service.documents().batchUpdate(documentId=document_id, body={'requests': requests}).execute()

def get_file_content(service, file_id):
    # Get file metadata to determine MIME type
    file = service.files().get(fileId=file_id, fields='mimeType, name').execute()
    mime_type = file.get('mimeType')
    file_name = file.get('name')

    if mime_type == 'text/plain':
        # For plain text files
        request = service.files().get_media(fileId=file_id)
        fh = io.BytesIO()
        downloader = MediaIoBaseDownload(fh, request)
        done = False
        while not done:
            status, done = downloader.next_chunk()
        content = fh.getvalue().decode('utf-8')
        return {'file_name': file_name, 'content': content}

    elif mime_type == 'application/vnd.google-apps.document':
        # For Google Docs, export as plain text
        request = service.files().export_media(fileId=file_id, mimeType='text/plain')
        fh = io.BytesIO()
        downloader = MediaIoBaseDownload(fh, request)
        done = False
        while not done:
            status, done = downloader.next_chunk()
        content = fh.getvalue().decode('utf-8')
        return {'file_name': file_name, 'content': content}

    elif mime_type in ['application/vnd.openxmlformats-officedocument.wordprocessingml.document',
                       'application/msword']:
        # For Word documents, download and extract text using python-docx
        request = service.files().get_media(fileId=file_id)
        fh = io.BytesIO()
        downloader = MediaIoBaseDownload(fh, request)
        done = False
        while not done:
            status, done = downloader.next_chunk()
        fh.seek(0)
        try:
            doc = Document(fh)
            fullText = []
            for para in doc.paragraphs:
                fullText.append(para.text)
            content = '\n'.join(fullText)
            return {'file_name': file_name, 'content': content}
        except Exception as e:
            print(f"Error processing Word document {file_name}: {e}")
            return {'file_name': file_name, 'content': None}

    else:
        print(f"Unsupported MIME type for file {file_name}: {mime_type}")
        return {'file_name': file_name, 'content': None}

def generate_AI_output(system_prompt, content):
    """Generate AI output using OpenAI API."""

    try:
        response_o1_mini = client.chat.completions.create(
          model="o1-mini-2024-09-12",
          messages=[
            {
              "role": "user",
              "content": [
                {
                  "type": "text",
                  "text": system_prompt + "Transcript: " + content
                }
              ]
            }
          ]
        )

        # Extract and display only the content of the assistant's message
        AI_output = response_o1_mini.choices[0].message.content
        log = f"model: ChatGPT-{response_o1_mini.model} \noutput_tokens: {response_o1_mini.usage.completion_tokens} \ninput_tokens: {response_o1_mini.usage.prompt_tokens} \ntotal_tokens: {response_o1_mini.usage.total_tokens} \nreasoning_tokens: {response_o1_mini.usage.completion_tokens_details.reasoning_tokens}"
    except Exception as e:
        print(f"Error generating AI output: {e}")
        return None

    return AI_output, log

def create_doc_with_text_in_folder(docs_service, drive_service, folder_id, title, text):
    try:
        # Create a new Google Doc in the specified folder
        file_metadata = {
            'name': title,
            'mimeType': 'application/vnd.google-apps.document',
            'parents': [folder_id]
        }
        file = drive_service.files().create(body=file_metadata, fields='id').execute()
        document_id = file.get('id')

        # Insert the text into the new document and update text style and paragraph alignment
        requests = [
            {
                'insertText': {
                    'location': {
                        'index': 1  # Start at position 1 to leave index 0 for document start
                    },
                    'text': text
                }
            },
            {
                'updateTextStyle': {
                    'range': {
                        'startIndex': 1,
                        'endIndex': 1 + len(text)
                    },
                    'textStyle': {
                        'weightedFontFamily': {
                            'fontFamily': 'Work Sans'
                        }
                    },
                    'fields': 'weightedFontFamily'
                }
            },
            {
                'updateParagraphStyle': {
                    'range': {
                        'startIndex': 1,
                        'endIndex': 1 + len(text)
                    },
                    'paragraphStyle': {
                        'alignment': 'JUSTIFIED'
                    },
                    'fields': 'alignment'
                }
            }
        ]
        docs_service.documents().batchUpdate(
            documentId=document_id,
            body={'requests': requests}
        ).execute()

        return document_id
    except Exception as e:
        raise Exception(f"Error creating or updating document: {e}\n")

def get_column_letter(n):
    """Converts a zero-based column index to a column letter."""
    string = ''
    n += 1  # Adjust for 1-based indexing
    while n > 0:
        n, remainder = divmod(n - 1, 26)
        string = chr(65 + remainder) + string
    return string


################################################################################


def update_worksheet(sheets_service, spreadsheet_id, sheet_name, primary_key_col_name, primary_key_value, status_col_name, new_value):
    try:
        # Define the range to fetch (from row 3 onwards)
        range_to_fetch = f"{sheet_name}!A3:ZZZ"

        # Get values from the worksheet starting from row 3
        worksheet = sheets_service.spreadsheets().values().get(
            spreadsheetId=spreadsheet_id,
            range=range_to_fetch
        ).execute()
        data = worksheet.get('values', [])
        if not data:
            print(f"The {sheet_name} worksheet is empty.")
            return

        # Extract the header row (from data[0], which corresponds to row 3)
        header = data[0]

        # Find the index of the primary_key column (case-insensitive)
        try:
            primary_key_col_index = next(
                i for i, col_name in enumerate(header)
                if col_name.strip().lower() == primary_key_col_name.strip().lower()
            )
        except StopIteration:
            raise Exception(f"Column '{primary_key_col_name}' not found in the header.")

        # Find the index of the status column (case-insensitive)
        try:
            status_col_index = next(
                i for i, col_name in enumerate(header)
                if col_name.strip().lower() == status_col_name.strip().lower()
            )
        except StopIteration:
            raise Exception(f"Column '{status_col_name}' not found in the header.")

        # Search for the primary_key_value in the data
        row_number = None
        for idx, row in enumerate(data[1:], start=4):  # data[1] corresponds to row 4 in the sheet
            if len(row) > primary_key_col_index:
                if row[primary_key_col_index].strip() == primary_key_value:
                    row_number = idx
                    break

        if row_number is None:
            print(f"Primary key '{primary_key_value}' not found in the sheet.")
            return

        # Convert column index to column letter
        column_letter = get_column_letter(status_col_index)
        cell_range = f"{sheet_name}!{column_letter}{row_number}"

        # Prepare the data to update
        body = {'values': [[new_value]]}

        # Update the cell value
        sheets_service.spreadsheets().values().update(
            spreadsheetId=spreadsheet_id,
            range=cell_range,
            valueInputOption='RAW',
            body=body
        ).execute()

        #print(f"Updated cell {cell_range} to '{new_value}'\n")

    except Exception as e:
        raise Exception(f"Error updating worksheet: {e}\n")

def main(file_ids):
    border = "=" * 70

    for file_id in file_ids:
        try:
            # Retrieve file content
            file_content = get_file_content(service=drive_service, file_id=file_id)
            trn_file_name = file_content["file_name"]  # e.g., "example trn"
            content = file_content["content"]

            # Generate AI outputs
            AI_output_1, _ = generate_AI_output(system_prompt=system_prompt_first_part, content=content)
            AI_output_2, log = generate_AI_output(system_prompt=system_prompt_second_part, content=content)

            # Format the document content
            format_docs = f"File Info: \n\nFile name: {trn_file_name} \nTranscript file url: \nhttps://docs.google.com/document/d/{file_id}/ \n\n{border} \n\n{AI_output_1} \n\n{border} \n\n{AI_output_2} \n\n{border} \n\nLog: \n\n{log}"

            # Extract the primary key (file name without " trn")
            file_name = trn_file_name[:-4]  # Remove " trn" or the last 4 characters
            sum_blog_file_name = file_name + " sum-blog"

            # Upload format_docs to a new Google Doc
            new_doc_id = create_doc_with_text_in_folder(
                docs_service=docs_service,
                drive_service=drive_service,
                folder_id=FOLDER_ID_SUM_BLOG,
                title=sum_blog_file_name,
                text=format_docs
            )

            doc_file_url = f"https://docs.google.com/document/d/{new_doc_id}/"

            # Update 'Pending' to doc_file_url in the spreadsheet
            update_worksheet(
                sheets_service=sheets_service,
                spreadsheet_id=SPREADSHEET_ID,
                sheet_name="List v2",
                primary_key_col_name="File Name",
                primary_key_value=file_name,
                status_col_name="Save to Blog post summary GDrive",
                new_value=doc_file_url
            )

            print(f"File processing done: {trn_file_name} | {file_id} | New Doc url: {doc_file_url}\n")

        except Exception as e:
            print(f"Error in main function: {e} | {file_id}\n")

"""# ***Outputs***"""

# Initialize credentials and services
docs_service, drive_service, sheets_service = initialize_google_creds()

# Find and display duplicate file names with filtering
_, _, primary_keys, _ = find_duplicate_primary_keys(
    sheets_service=sheets_service,
    spreadsheet_id=SPREADSHEET_ID,
    range_name="List v2",
    primary_key_col_name="File Name",
    filter_col_name="Save to Blog post summary GDrive",
    filter_value="Pending"
)

gdrive_files = get_files_in_folder(
    drive_service=drive_service,
    GDRIVE_FOLDER_ID=FOLDER_ID_TRN
)

find_duplicate_files(
    drive_service=drive_service,
    GDRIVE_FOLDER_ID=FOLDER_ID_TRN,
    files=gdrive_files
)

# Call the function to check transcript file names
file_ids = check_transcript_file_names(
    primary_keys=primary_keys,
    gdrive_files=gdrive_files,
)

# Call the updated function with file_ids
process_files_in_folder(
    drive_service=drive_service,
    docs_service=docs_service,
    folder_id=FOLDER_ID_TRN,
    gdrive_files=gdrive_files,
    file_ids=file_ids
)

main(file_ids=file_ids)